{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset repo\n",
    "!git reset\n",
    "!git clean -fd\n",
    "!docker stop $(docker ps -a -q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things clean and not run into version conflicts its recommended to setup a new anaconda environment with this command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Created with conda env export > environment.yml\n",
    "!conda env create -f environment.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you just use an existing python environment, you can also easily install all three via pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install mlflow\n",
    "! pip install dvc\n",
    "! pip install sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preperations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the libraries we need (this is actually the default import I load via jupyter-magic commands every time I start something new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load ~/dev/imports.py\n",
    "import os\n",
    "from datetime import datetime as dt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "import IPython.display as ipd\n",
    "\n",
    "# Set random seed \n",
    "RSEED = 42\n",
    "\n",
    "# Visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = (25, 5)\n",
    "%matplotlib inline\n",
    "plt.style.use('fivethirtyeight')\n",
    "plt.rcParams['font.size'] = 18\n",
    "\n",
    "import seaborn as sns\n",
    "cm = sns.light_palette(\"green\", as_cmap=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I created some helper functions we will need, but since they do not support the understanding of the frameworks we want to learn about, I excluded them into an own python file. Feels free to check them out, if you want to dig deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the boston housing prices dataset which can be received directly from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "\n",
    "data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "data['target'] = pd.Series(boston.target)\n",
    "data.sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the basic code we would use, if we do not want to track any information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "# Do a train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "\n",
    "# Create and fit regression\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(x_train, y_train)\n",
    "\n",
    "# Do prediction and calculate mean absolute error\n",
    "test_pred = linreg.predict(x_test)\n",
    "mean_absolute_error(y_test, test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to use some visualization, we will first start a database as storage backend and a visualzation tool using docker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "docker run -d --rm -p 27017:27017 --name mongodb mongo\n",
    "docker run -d --rm -p 9000:9000 --name omniboard --link mongodb:mongo vivekratnavel/omniboard -m mongo:27017:sacred\n",
    "echo \"wait a few seconds till containers are up\"\n",
    "sleep 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open http://127.0.0.1:9000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v1 (the more explicit version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sacred_simple.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "from sacred import Experiment\n",
    "\n",
    "ex = Experiment('Boston Housing Prices')\n",
    "from utils import *\n",
    "        \n",
    "@ex.capture\n",
    "def capturestuff(_seed):\n",
    "    print(_seed)    \n",
    "\n",
    "def cfg():\n",
    "    params = {\n",
    "        \"objective\" : \"regression\",\n",
    "        \"metric\" : \"rmse\", \n",
    "        \"alpha\" : 0.5,\n",
    "        \"min_child_samples\" : 10,\n",
    "        \"learning_rate\" : 0.1,\n",
    "        \"bagging_fraction\" : 0.5,\n",
    "        \"feature_fraction\" : 0.5,\n",
    "        \"bagging_frequency\" : 10\n",
    "    }\n",
    "    return params\n",
    "    \n",
    "def logSacred(run,model,data,param=dict(),metrics=dict(),features=None, tags=dict()):\n",
    "    # Imports\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    # Get some general information\n",
    "    output_folder = \"sacred_out\"\n",
    "    type = model.__module__.split(\".\")[0]\n",
    "    modelname = model.__class__.__name__\n",
    "    \n",
    "    # Track config\n",
    "    mycfg=cfg()\n",
    "    ex.add_config(cfg_or_file=mycfg)\n",
    "    \n",
    "    # Track dependencies\n",
    "    import pkg_resources\n",
    "    for d in pkg_resources.working_set:\n",
    "        ex.add_package_dependency(d.project_name,d.version)\n",
    "    \n",
    "    # Track source code\n",
    "    data.to_csv(\"{}/data\".format(output_folder))\n",
    "    ex.add_resource(\"{}/data\".format(output_folder))\n",
    "    \n",
    "    # Create file about features\n",
    "    if features is not None:\n",
    "        with open(\"{}/features.txt\".format(output_folder), \"w+\") as f: \n",
    "            f.write(\",\".join(features))\n",
    "        ex.add_artifact(\"{}/features.txt\".format(output_folder))\n",
    "        \n",
    "    # plot Feature importances if avaible\n",
    "    if plotFeatureImportances(model, features, type):\n",
    "        ex.add_artifact(\"{}/featureimportance.png\".format(output_folder))\n",
    "\n",
    "    # Track Model binary\n",
    "    if type==\"sklearn\":\n",
    "        _ = joblib.dump(model,\"{}/sklearn\".format(output_folder))\n",
    "        ex.add_artifact(\"{}/sklearn\".format(output_folder))\n",
    "    if type==\"lgb\":\n",
    "        model.save_model(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        ex.add_artifact(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        \n",
    "    # Log metrics\n",
    "    for k,v in metrics.items():\n",
    "        ex.log_scalar(k,v)\n",
    "        \n",
    "    # Tags can only be set using the UI\n",
    "\n",
    "@ex.automain\n",
    "def run(_run):\n",
    "    \"\"\"\n",
    "    f a nice greet message.\n",
    "\n",
    "    Uses the name from config.\n",
    "    \"\"\"\n",
    "    # Setup\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "    # Do a train_test_split on my Data\n",
    "    data = getData()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "    \n",
    "    # Define my params\n",
    "    params=dict(alpha=0.5)\n",
    "    \n",
    "    clf = Lasso(**params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    metrics = eval_metrics(y_test, predictions)\n",
    "        \n",
    "    logSacred(_run,clf,data,param=params,metrics=metrics,features=x_test.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p sacred_out\n",
    "!python sacred_simple.py -m sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### v2 (the more sacred-style ways)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile sacred_simple2.py\n",
    "#!/usr/bin/env python\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "from sacred import Experiment\n",
    "\n",
    "# Imports need to be done in the beginning of the file, since sacred won't recognize them, if they occur within a function\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "ex = Experiment('Boston Housing Prices')\n",
    "from utils import *\n",
    "        \n",
    "@ex.capture\n",
    "def capturestuff(_seed):\n",
    "    print(_seed)\n",
    "\n",
    "def getData():\n",
    "    from sklearn.datasets import load_boston\n",
    "    boston = load_boston()\n",
    "\n",
    "    data = pd.DataFrame(boston.data,columns=boston.feature_names)\n",
    "    data['target'] = pd.Series(boston.target)\n",
    "    return data\n",
    "\n",
    "@ex.config\n",
    "def cfg(_log):\n",
    "    alpha= 0.5\n",
    "    \n",
    "def logSacred(run,model,data,output_folder=\"sacred_out\", param=dict(),metrics=dict(),features=None, tags=dict()):\n",
    "    # Get some general information\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    type = model.__module__.split(\".\")[0]\n",
    "    modelname = model.__class__.__name__\n",
    "    \n",
    "    # Config will be tracked automatically\n",
    "    \n",
    "    # Dependencies will also be tracked automatically\n",
    "    \n",
    "    # Track source code\n",
    "    data.to_csv(\"{}/data\".format(output_folder))\n",
    "    #ex.add_source_file(\"{}/data\".format(output_folder))\n",
    "    ab = ex.open_resource(\"{}/data\".format(output_folder))\n",
    "    # Create file about features\n",
    "    if features is not None:\n",
    "        with open(\"{}/features.txt\".format(output_folder), \"w+\") as f: \n",
    "            f.write(\",\".join(features))\n",
    "        ex.add_artifact(\"{}/features.txt\".format(output_folder))\n",
    "        \n",
    "    # plot Feature importances if avaible\n",
    "    if plotFeatureImportances(model, features, type):\n",
    "        ex.add_artifact(\"{}/featureimportance.png\".format(output_folder))\n",
    "\n",
    "    # Track Model binary\n",
    "    if type==\"sklearn\":\n",
    "        _ = joblib.dump(model,\"{}/sklearn\".format(output_folder))\n",
    "        ex.add_artifact(\"{}/sklearn\".format(output_folder))\n",
    "    if type==\"lgb\":\n",
    "        model.save_model(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        ex.add_artifact(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        \n",
    "    # Log metrics\n",
    "    for k,v in metrics.items():\n",
    "        ex.log_scalar(k,v)\n",
    "        \n",
    "    # Set some tags to identify the experiment\n",
    "    for tag, v in tags.items():\n",
    "        ex.add.set_tag(t,v)\n",
    "\n",
    "@ex.automain\n",
    "def run(_run, alpha):\n",
    "    # Setup\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "    # Do a train_test_split on my Data\n",
    "    data = getData()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "    \n",
    "    # Define my params\n",
    "    params=dict(alpha=alpha)\n",
    "    \n",
    "    clf = Lasso(**params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    metrics = eval_metrics(y_test, predictions)\n",
    "        \n",
    "    logSacred(_run,clf,data,param=params,metrics=metrics,features=x_test.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p sacred_out\n",
    "!python sacred_simple2.py -m sacred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And provide some additional parameters to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sacred_simple2.py -m sacred with 'alpha=0.2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sacred also offers some functions to print configurations or list dependencies it detected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sacred_simple2.py print_config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sacred_simple2.py print_dependencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can easily switch the storage backend between runs using --file_storage handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python sacred_simple.py --file_storage=BASEDIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DVC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC works much like git, so we will first need to init the project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC creates a .dvc folder that stores all important information and can be tracked using git (just as mentioned above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! git status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# To re-run the command above, remove the dvc file  as well as the .dvc folder\n",
    "#rm -rf .dvc\n",
    "#rm simple.dvc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DVC does not provide a python api but gets controlled using the command line and scripts executes. Therefore we use the jupyter %writefile magic to create the file for our training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dvc_simple.py\n",
    "def logDVC(model,output_folder=\"dvc_out\", param=dict(),metrics=dict(),features=None, tags=dict()):\n",
    "    import json\n",
    "    from sklearn.externals import joblib\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Get some general information\n",
    "    type = model.__module__.split(\".\")[0]\n",
    "\n",
    "    # No option to set some tags to identify the experiment\n",
    "\n",
    "    # Save Model\n",
    "    if type==\"sklearn\":\n",
    "        _ = joblib.dump(model,\"{}/mymodel\".format(output_folder))\n",
    "    if type==\"lgb\":\n",
    "        model.save_model(\"{}/mymodel\".format(output_folder))\n",
    "\n",
    "    # Log metrics\n",
    "    with open('{}/metrics.txt'.format(output_folder), 'w') as f:\n",
    "        f.write(json.dumps(metrics))\n",
    "\n",
    "    # plot Feature importances if avaible\n",
    "    plotFeatureImportances(model, features, type)\n",
    "\n",
    "    # Create file about features\n",
    "    if features is not None:\n",
    "        with open(\"{}/features.txt\".format(output_folder), \"w+\") as f: \n",
    "            f.write(\",\".join(features))\n",
    "            \n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import Lasso\n",
    "    # We need to import utils here, since it is an own script and the execution environment has no access to the jupyter execution environment\n",
    "    from utils import *\n",
    "\n",
    "    # Do a train_test_split\n",
    "    data = getData()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "\n",
    "    # Define the details of our run\n",
    "    params=dict(alpha=0.4)\n",
    "    clf = Lasso(**params)\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "\n",
    "    metrics = eval_metrics(y_test, predictions)\n",
    "    logDVC(clf,param=params,metrics=metrics,features=x_test.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can run the command using DVC. Since it will need a couple of information, its help fucntion is an important reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dvc run --help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We tell DVC which file to run (-d), the project file that keeps the information (-f), the metrics (-M) and the output file (-o)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $(pwd)\n",
    "dvc run  \\\n",
    "  -d dvc_simple.py \\\n",
    "  -f simple.dvc \\\n",
    "  -o dvc_count\n",
    "  python dvc_simple.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc pipeline show simple.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! dvc repro simple.dvc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git diff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLFlow offers a visualization server, but since this is a long running process and will block other notebook cells start it from a terminal using:\n",
    "\n",
    "```\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "After a few seconds you can open it in your Browser: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!open http://127.0.0.1:5000/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a function to do the logging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import git\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "sha = repo.head.object.hexsha\n",
    "repo.remotes.origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGitInfos():\n",
    "    import git\n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    sha = repo.head.object.hexsha\n",
    "    try:\n",
    "        remoteurl = repo.remotes.origin.url\n",
    "    except AttributeError:\n",
    "        remoteurl = \"\"\n",
    "    return sha, remoteurl\n",
    "\n",
    "def logMlflow(model,data,output_folder=\"mlflow_out\", param=dict(),metrics=dict(),features=None, tags=dict(),run_name=None):\n",
    "    # Imports\n",
    "    from sklearn.externals import joblib\n",
    "    import mlflow\n",
    "    import os\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    \n",
    "    # Get some general information\n",
    "    type = model.__module__.split(\".\")[0]\n",
    "    modelname = model.__class__.__name__\n",
    "    sha, remoteurl = getGitInfos()\n",
    "    \n",
    "    # Start actual logging\n",
    "    mlflow.set_experiment(experiment_name=\"demo\")\n",
    "    if not run_name:\n",
    "        run_name = modelname\n",
    "    with mlflow.start_run(source_name=remoteurl,source_version=sha, run_name=run_name):\n",
    "        \n",
    "        # Log Parameters\n",
    "        for k,v in param.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # Track dependencies\n",
    "        import pkg_resources\n",
    "        with open(\"{}/dependencies.txt\".format(output_folder), \"w+\") as f: \n",
    "            for d in pkg_resources.working_set:\n",
    "                f.write(\"{}={}\\n\".format(d.project_name,d.version))\n",
    "        mlflow.log_artifact(\"{}/dependencies.txt\".format(output_folder))\n",
    "        \n",
    "        # Track data\n",
    "        data.to_csv(\"{}/data\".format(output_folder))\n",
    "        mlflow.log_artifact(\"{}/data\".format(output_folder))\n",
    "        \n",
    "        if type==\"sklearn\":\n",
    "            _ = joblib.dump(model,\"{}/sklearn\".format(output_folder))\n",
    "            mlflow.log_artifact(\"{}/sklearn\".format(output_folder))\n",
    "        if type==\"lgb\":\n",
    "            model.save_model(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "            mlflow.log_artifact(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        \n",
    "        # Log metrics\n",
    "        for k,v in metrics.items():\n",
    "            mlflow.log_metric(k,v)\n",
    "\n",
    "        # plot Feature importances if avaible\n",
    "        featurePlot = plotFeatureImportances(model, features, type)\n",
    "        if featurePlot:\n",
    "            mlflow.log_artifact(\"{}.png\".format(featurePlot))\n",
    "            \n",
    "        # Set some tags to identify the experiment\n",
    "        mlflow.set_tag(\"model\",modelname)\n",
    "        for tag, v in tags.items():\n",
    "            mlflow.set_tag(t,v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use it after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "from utils import *\n",
    "\n",
    "# Do a train_test_split\n",
    "data = getData()\n",
    "x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "\n",
    "params=dict(alpha=0.1)\n",
    "\n",
    "clf = Lasso(**params)\n",
    "\n",
    "def run(clf, params, run_name=None):\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    metrics = eval_metrics(y_test, predictions)\n",
    "    print(metrics['mae'], metrics['r2'])\n",
    "\n",
    "    logMlflow(clf,data,param=params,metrics=metrics,features=x_test.columns.values, run_name=run_name)\n",
    "    \n",
    "run(clf,params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is easy to test different models and parameter combinations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "from sklearn.linear_model import Lasso\n",
    "params = [\n",
    "    dict(alpha=0.4),\n",
    "    dict(alpha=0.3),\n",
    "    dict(alpha=0.2)\n",
    "]\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    print(p)\n",
    "    clf = Lasso(**p)\n",
    "    run(clf,p, run_name =  clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "params = [\n",
    "    dict(alpha=0.1),\n",
    "    dict(alpha=0.5),\n",
    "    dict(alpha=0.9)\n",
    "]\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    print(p)\n",
    "    clf = Ridge(**p)\n",
    "    run(clf,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "params = [\n",
    "    dict(alpha=0.1, l1_ratio=0.5),\n",
    "    dict(alpha=0.5, l1_ratio=0.5),\n",
    "    dict(alpha=0.9, l1_ratio=0.5),\n",
    "    dict(alpha=0.9, l1_ratio=0.2),\n",
    "    dict(alpha=0.9, l1_ratio=0.8)\n",
    "]\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    print(p)\n",
    "    clf = ElasticNet(**p)\n",
    "    run(clf,p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "params = [\n",
    "    dict(max_depth=6, min_samples_split=5),\n",
    "    dict(max_depth=5, min_samples_split=3),\n",
    "    dict(max_depth=4, min_samples_split=5),\n",
    "]\n",
    "\n",
    "for i, p in enumerate(params):\n",
    "    print(p)\n",
    "    clf = DecisionTreeRegressor(**p)\n",
    "    run(clf,p, run_name =  clf.__class__.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLFlow Project\n",
    "\n",
    "To make the code above a MLFlow Project and use its remote-run functionality, we will first need to create a file named MLProject which speciefies the environment file as well as the entrypoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mlflow/MLProject\n",
    "name: mlflow\n",
    "\n",
    "conda_env: environment.yml\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    parameters:\n",
    "      alpha: float\n",
    "      l1_ratio: {type: float, default: 0.1}\n",
    "    command: \"python train.py {alpha} {l1_ratio}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also we will need to make a few adaptions to the code above:\n",
    " * import the utils functionalities\n",
    " * define a [main functionality](https://stackoverflow.com/questions/419163/what-does-if-name-main-do)\n",
    " * parse the command line arguments using sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile mlflow/train.py\n",
    "from utils import *\n",
    "import git\n",
    "def logMlflow(model,data,param=dict(),metrics=dict(),features=None, tags=dict(),run_name=None):\n",
    "    # Imports\n",
    "    import mlflow\n",
    "    import os\n",
    "    from sklearn.externals import joblib\n",
    "    \n",
    "    # Get some general information\n",
    "    output_folder = \"mlflow_out\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    type = model.__module__.split(\".\")[0]\n",
    "    modelname = model.__class__.__name__\n",
    "    \n",
    "    # Start actual logging\n",
    "    \n",
    "    repo = git.Repo(search_parent_directories=True)\n",
    "    sha = repo.head.object.hexsha\n",
    "    mlflow.set_experiment(experiment_name=\"demo\")\n",
    "    if not run_name:\n",
    "        run_name = modelname\n",
    "    with mlflow.start_run(source_name=repo.remotes.origin.url,source_version=sha, run_name=run_name):\n",
    "        \n",
    "        # Log Parameters\n",
    "        for k,v in param.items():\n",
    "            mlflow.log_param(k, v)\n",
    "\n",
    "        # Track dependencies\n",
    "        import pkg_resources\n",
    "        with open(\"{}/dependencies.txt\".format(output_folder), \"w+\") as f: \n",
    "            for d in pkg_resources.working_set:\n",
    "                f.write(\"{}={}\\n\".format(d.project_name,d.version))\n",
    "        mlflow.log_artifact(\"{}/dependencies.txt\".format(output_folder))\n",
    "        \n",
    "        # Track data\n",
    "        data.to_csv(\"{}/data\".format(output_folder))\n",
    "        mlflow.log_artifact(\"{}/data\".format(output_folder))\n",
    "        \n",
    "        if type==\"sklearn\":\n",
    "            _ = joblib.dump(model,\"{}/sklearn\".format(output_folder))\n",
    "            mlflow.log_artifact(\"{}/sklearn\".format(output_folder))\n",
    "        if type==\"lgb\":\n",
    "            model.save_model(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "            mlflow.log_artifact(\"{}/lghtgbm.txt\".format(output_folder))\n",
    "        \n",
    "        # Log metrics\n",
    "        for k,v in metrics.items():\n",
    "            mlflow.log_metric(k,v)\n",
    "\n",
    "        # plot Feature importances if avaible\n",
    "        featurePlot = plotFeatureImportances(model, features, type)\n",
    "        if featurePlot:\n",
    "            mlflow.log_artifact(\"{}.png\".format(featurePlot))\n",
    "            \n",
    "        # Set some tags to identify the experiment\n",
    "        mlflow.set_tag(\"model\",modelname)\n",
    "        for tag, v in tags.items():\n",
    "            mlflow.set_tag(t,v)\n",
    "            \n",
    "def run(clf, params, run_name=None):\n",
    "    clf.fit(x_train, y_train)\n",
    "    predictions = clf.predict(x_test)\n",
    "    metrics = eval_metrics(y_test, predictions)\n",
    "    print(metrics['mae'], metrics['r2'])\n",
    "\n",
    "    logMlflow(clf,data,param=params,metrics=metrics,features=x_test.columns.values, run_name=run_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.linear_model import ElasticNet\n",
    "    import sys\n",
    "    # Do a train_test_split\n",
    "    data = getData()\n",
    "    x_train, x_test, y_train, y_test = train_test_split(data.iloc[:,:-1], data.iloc[:,-1], test_size=10, random_state=42)\n",
    "    \n",
    "    params=dict(alpha=float(sys.argv[1]) if len(sys.argv) > 1 else 0.5,\n",
    "                l1_ratio = float(sys.argv[2]) if len(sys.argv) > 2 else 0.5)\n",
    "\n",
    "    clf = ElasticNet(**params)\n",
    "\n",
    "    run(clf,params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should be able to run this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow run mlflow -P alpha=0.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or run it directly from our github account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mlflow run mlflow -P alpha=0.4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
